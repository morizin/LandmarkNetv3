{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba6ceb8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:06:55.889563Z",
     "iopub.status.busy": "2021-10-01T22:06:55.887812Z",
     "iopub.status.idle": "2021-10-01T22:06:55.890836Z",
     "shell.execute_reply": "2021-10-01T22:06:55.891431Z",
     "shell.execute_reply.started": "2021-10-01T21:29:23.920269Z"
    },
    "papermill": {
     "duration": 0.059207,
     "end_time": "2021-10-01T22:06:55.891736",
     "exception": false,
     "start_time": "2021-10-01T22:06:55.832529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install ../input/kerasapplications/ > /dev/null\n",
    "# !pip install ../input/efficientnet-keras-source-code/ > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ebbab38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:06:56.032969Z",
     "iopub.status.busy": "2021-10-01T22:06:56.031783Z",
     "iopub.status.idle": "2021-10-01T22:06:56.035376Z",
     "shell.execute_reply": "2021-10-01T22:06:56.034788Z",
     "shell.execute_reply.started": "2021-10-01T21:29:23.927207Z"
    },
    "papermill": {
     "duration": 0.076124,
     "end_time": "2021-10-01T22:06:56.035531",
     "exception": false,
     "start_time": "2021-10-01T22:06:55.959407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cv = {'igm':[{'id':'hello', 'target':'1', 'family':1},{'id':'hello', 'target':'1', 'family':2}, {'id':'hello', 'target':'1', 'family':4}, {'id':'hello', 'target':'1', 'family':3}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37c4bedf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:06:56.123723Z",
     "iopub.status.busy": "2021-10-01T22:06:56.122734Z",
     "iopub.status.idle": "2021-10-01T22:06:56.126010Z",
     "shell.execute_reply": "2021-10-01T22:06:56.125426Z",
     "shell.execute_reply.started": "2021-10-01T21:29:23.935976Z"
    },
    "papermill": {
     "duration": 0.048488,
     "end_time": "2021-10-01T22:06:56.126155",
     "exception": false,
     "start_time": "2021-10-01T22:06:56.077667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for igm , pr in cv.items():\n",
    "#     cv[igm] = [sorted(pr[i].items(), key = lambda x: x['family']) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fdd87ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:06:56.223555Z",
     "iopub.status.busy": "2021-10-01T22:06:56.219318Z",
     "iopub.status.idle": "2021-10-01T22:06:56.225975Z",
     "shell.execute_reply": "2021-10-01T22:06:56.226569Z",
     "shell.execute_reply.started": "2021-10-01T21:29:24.272733Z"
    },
    "papermill": {
     "duration": 0.058453,
     "end_time": "2021-10-01T22:06:56.226742",
     "exception": false,
     "start_time": "2021-10-01T22:06:56.168289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import os\n",
    "# import math\n",
    "# import random\n",
    "# import re\n",
    "# import warnings\n",
    "# from pathlib import Path\n",
    "# from PIL import Image\n",
    "# from typing import Optional, Tuple\n",
    "\n",
    "# import efficientnet.tfkeras as efn\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from scipy import spatial\n",
    "# from sklearn.preprocessing import normalize\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# DATADIR = Path(\"../input/landmark-retrieval-2021/\")\n",
    "# TEST_IMAGE_DIR = DATADIR / \"test\"\n",
    "# TRAIN_IMAGE_DIR = DATADIR / \"index\"\n",
    "\n",
    "# N_CLASSES = 81313\n",
    "\n",
    "# import time\n",
    "\n",
    "# from contextlib import contextmanager\n",
    "\n",
    "\n",
    "# @contextmanager\n",
    "# def timer(name):\n",
    "#     t0 = time.time()\n",
    "#     print(f\"[{name}]\")\n",
    "#     yield\n",
    "#     print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "# def set_seed(seed=42):\n",
    "#     random.seed(seed)\n",
    "#     os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "# set_seed(1213)\n",
    "# def auto_select_accelerator():\n",
    "#     try:\n",
    "#         tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "#         tf.config.experimental_connect_to_cluster(tpu)\n",
    "#         tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "#         strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "#         print(\"Running on TPU:\", tpu.master())\n",
    "#     except ValueError:\n",
    "#         strategy = tf.distribute.get_strategy()\n",
    "#     print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n",
    "#     return strategy\n",
    "# strategy = auto_select_accelerator()\n",
    "# REPLICAS = strategy.num_replicas_in_sync\n",
    "# AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# class GeM(tf.keras.layers.Layer):\n",
    "#     def __init__(self, pool_size, init_norm=3.0, normalize=False, **kwargs):\n",
    "#         self.pool_size = pool_size\n",
    "#         self.init_norm = init_norm\n",
    "#         self.normalize = normalize\n",
    "\n",
    "#         super(GeM, self).__init__(**kwargs)\n",
    "\n",
    "#     def get_config(self):\n",
    "#         config = super().get_config().copy()\n",
    "#         config.update({\n",
    "#             'pool_size': self.pool_size,\n",
    "#             'init_norm': self.init_norm,\n",
    "#             'normalize': self.normalize,\n",
    "#         })\n",
    "#         return config\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         feature_size = input_shape[-1]\n",
    "#         self.p = self.add_weight(name='norms', shape=(feature_size,),\n",
    "#                                  initializer=tf.keras.initializers.constant(self.init_norm),\n",
    "#                                  trainable=True)\n",
    "#         super(GeM, self).build(input_shape)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         x = inputs\n",
    "#         x = tf.math.maximum(x, 1e-6)\n",
    "#         x = tf.pow(x, self.p)\n",
    "\n",
    "#         x = tf.nn.avg_pool(x, self.pool_size, self.pool_size, 'VALID')\n",
    "#         x = tf.pow(x, 1.0 / self.p)\n",
    "\n",
    "#         if self.normalize:\n",
    "#             x = tf.nn.l2_normalize(x, 1)\n",
    "#         return x\n",
    "\n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         return tuple([None, input_shape[-1]])\n",
    "# class ArcMarginProduct(tf.keras.layers.Layer):\n",
    "#     '''\n",
    "#     Implements large margin arc distance.\n",
    "\n",
    "#     Reference:\n",
    "#         https://arxiv.org/pdf/1801.07698.pdf\n",
    "#         https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n",
    "#             blob/master/src/modeling/metric_learning.py\n",
    "#     '''\n",
    "#     def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n",
    "#                  ls_eps=0.0, **kwargs):\n",
    "\n",
    "#         super(ArcMarginProduct, self).__init__(**kwargs)\n",
    "\n",
    "#         self.n_classes = n_classes\n",
    "#         self.s = s\n",
    "#         self.m = m\n",
    "#         self.ls_eps = ls_eps\n",
    "#         self.easy_margin = easy_margin\n",
    "#         self.cos_m = tf.math.cos(m)\n",
    "#         self.sin_m = tf.math.sin(m)\n",
    "#         self.th = tf.math.cos(math.pi - m)\n",
    "#         self.mm = tf.math.sin(math.pi - m) * m\n",
    "\n",
    "#     def get_config(self):\n",
    "\n",
    "#         config = super().get_config().copy()\n",
    "#         config.update({\n",
    "#             'n_classes': self.n_classes,\n",
    "#             's': self.s,\n",
    "#             'm': self.m,\n",
    "#             'ls_eps': self.ls_eps,\n",
    "#             'easy_margin': self.easy_margin,\n",
    "#         })\n",
    "#         return config\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         super(ArcMarginProduct, self).build(input_shape[0])\n",
    "\n",
    "#         self.W = self.add_weight(\n",
    "#             name='W',\n",
    "#             shape=(int(input_shape[0][-1]), self.n_classes),\n",
    "#             initializer='glorot_uniform',\n",
    "#             dtype='float32',\n",
    "#             trainable=True,\n",
    "#             regularizer=None)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         X, y = inputs\n",
    "#         y = tf.cast(y, dtype=tf.int32)\n",
    "#         cosine = tf.matmul(\n",
    "#             tf.math.l2_normalize(X, axis=1),\n",
    "#             tf.math.l2_normalize(self.W, axis=0)\n",
    "#         )\n",
    "#         sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n",
    "#         phi = cosine * self.cos_m - sine * self.sin_m\n",
    "#         if self.easy_margin:\n",
    "#             phi = tf.where(cosine > 0, phi, cosine)\n",
    "#         else:\n",
    "#             phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n",
    "#         one_hot = tf.cast(\n",
    "#             tf.one_hot(y, depth=self.n_classes),\n",
    "#             dtype=cosine.dtype\n",
    "#         )\n",
    "#         if self.ls_eps > 0:\n",
    "#             one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n",
    "\n",
    "#         output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "#         output *= self.s\n",
    "#         return output\n",
    "# def build_model(size=256, efficientnet_size=0, weights=\"imagenet\", count=0):\n",
    "#     inp = tf.keras.layers.Input(shape=(size, size, 3), name=\"inp1\")\n",
    "#     label = tf.keras.layers.Input(shape=(), name=\"inp2\")\n",
    "#     x = getattr(efn, f\"EfficientNetB{efficientnet_size}\")(\n",
    "#         weights=weights, include_top=False, input_shape=(size, size, 3))(inp)\n",
    "#     x = GeM(8)(x)\n",
    "#     x = tf.keras.layers.Flatten()(x)\n",
    "#     x = tf.keras.layers.Dense(512, name=\"dense_before_arcface\", kernel_initializer=\"he_normal\")(x)\n",
    "#     x = tf.keras.layers.BatchNormalization()(x)\n",
    "#     x = ArcMarginProduct(\n",
    "#         n_classes=N_CLASSES,\n",
    "#         s=30,\n",
    "#         m=0.5,\n",
    "#         name=\"head/arc_margin\",\n",
    "#         dtype=\"float32\"\n",
    "#     )([x, label])\n",
    "#     output = tf.keras.layers.Softmax(dtype=\"float32\")(x)\n",
    "#     model = tf.keras.Model(inputs=[inp, label], outputs=[output])\n",
    "#     opt = tf.optimizers.Adam(learning_rate=1e-4)\n",
    "#     model.compile(\n",
    "#         optimizer=opt,\n",
    "#         loss=[tf.keras.losses.SparseCategoricalCrossentropy()],\n",
    "#         metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "#     )\n",
    "#     return model\n",
    "# def create_model_for_inference(weights_path: str):\n",
    "#     with strategy.scope():\n",
    "#         base_model = build_model(\n",
    "#             size=256,\n",
    "#             efficientnet_size=7,\n",
    "#             weights=None,\n",
    "#             count=0)\n",
    "#         base_model.load_weights(weights_path)\n",
    "#         model = tf.keras.Model(inputs=base_model.get_layer(\"inp1\").input,\n",
    "#                                outputs=base_model.get_layer(\"dense_before_arcface\").output)\n",
    "#         return model\n",
    "# # Feature Extraction\n",
    "\n",
    "# def to_hex(image_id) -> str:\n",
    "#     return '{0:0{1}x}'.format(image_id, 16)\n",
    "\n",
    "\n",
    "# def get_image_path(subset, image_id):\n",
    "#     name = to_hex(image_id)\n",
    "#     return os.path.join(DATASET_DIR, subset, name[0], name[1], name[2], '{}.jpg'.format(name))\n",
    "\n",
    "\n",
    "# def load_image_tensor(image_path):\n",
    "#     tensor = tf.convert_to_tensor(np.array(Image.open(image_path).convert(\"RGB\")))\n",
    "#     tensor = tf.image.resize(tensor, size=(256, 256))\n",
    "#     tensor = tf.expand_dims(tensor, axis=0)\n",
    "#     return tf.cast(tensor, tf.float32) / 255.0\n",
    "\n",
    "\n",
    "# def create_batch(files):\n",
    "#     images = []\n",
    "#     for f in files:\n",
    "#         images.append(load_image_tensor(f))\n",
    "#     return tf.concat(images, axis=0)\n",
    "\n",
    "# def extract_global_features_tf(df, image_root_dir, n_models=4):\n",
    "#     image_paths = []\n",
    "#     for i in range(len(df)):\n",
    "#         tmp = df.loc[i,'id']\n",
    "#         image_paths.append(os.path.join(image_root_dir, tmp[0], tmp[1],tmp[2],tmp+'.jpg'))\n",
    "        \n",
    "# #     for root, dirs, files in os.walk(image_root_dir):\n",
    "# #         for file in files:\n",
    "# #             if file.endswith('.jpg'):\n",
    "# #                  image_paths.append(os.path.join(root, file))\n",
    "                    \n",
    "#     num_embeddings = len(image_paths)\n",
    "\n",
    "#     ids = num_embeddings * [None]\n",
    "#     ids = []\n",
    "#     for path in image_paths:\n",
    "#         ids.append(path.split('/')[-1][:-4])\n",
    "    \n",
    "#     embeddings = np.zeros((num_embeddings, 512))\n",
    "#     image_paths = np.array(image_paths)\n",
    "#     chunk_size = 512\n",
    "    \n",
    "#     n_chunks = len(image_paths) // chunk_size\n",
    "#     if len(image_paths) % chunk_size != 0:\n",
    "#         n_chunks += 1\n",
    "\n",
    "#     for n in range(n_models):\n",
    "#         print(f\"Getting Embedding for fold{n} model.\")\n",
    "#         model = create_model_for_inference(f\"../input/glret21-effb7/b7_fold{n}.h5\")\n",
    "#         for i in tqdm(range(n_chunks)):\n",
    "#             files = image_paths[i * chunk_size:(i + 1) * chunk_size]\n",
    "#             batch = create_batch(files)\n",
    "#             embedding_tensor = model.predict(batch)\n",
    "#             embeddings[i * chunk_size:(i + 1) * chunk_size] += embedding_tensor / n_models\n",
    "#         del model\n",
    "#         gc.collect()\n",
    "#         tf.keras.backend.clear_session()\n",
    "\n",
    "#     embeddings = normalize(embeddings, axis=1)\n",
    "\n",
    "#     return ids, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0020955a",
   "metadata": {
    "papermill": {
     "duration": 0.075058,
     "end_time": "2021-10-01T22:06:56.345225",
     "exception": false,
     "start_time": "2021-10-01T22:06:56.270167",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fafdaade",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:06:56.471265Z",
     "iopub.status.busy": "2021-10-01T22:06:56.469673Z",
     "iopub.status.idle": "2021-10-01T22:06:56.474251Z",
     "shell.execute_reply": "2021-10-01T22:06:56.473694Z",
     "shell.execute_reply.started": "2021-10-01T21:29:24.287748Z"
    },
    "papermill": {
     "duration": 0.059793,
     "end_time": "2021-10-01T22:06:56.474398",
     "exception": false,
     "start_time": "2021-10-01T22:06:56.414605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\n",
    "\n",
    "class Config:\n",
    "    batch_size = 64\n",
    "    num_workers = 2\n",
    "    num_to_rerank = 50\n",
    "    seed = 717171\n",
    "    embed_dim = 1024\n",
    "    CACHE_DIR = '/kaggle/temp/'\n",
    "    \n",
    "hyperparameters = Config()\n",
    "os.makedirs(hyperparameters.CACHE_DIR, exist_ok = True)\n",
    "TOPK = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82daff4b",
   "metadata": {
    "papermill": {
     "duration": 0.040861,
     "end_time": "2021-10-01T22:06:56.556498",
     "exception": false,
     "start_time": "2021-10-01T22:06:56.515637",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4455691f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:06:56.649157Z",
     "iopub.status.busy": "2021-10-01T22:06:56.648269Z",
     "iopub.status.idle": "2021-10-01T22:08:00.773643Z",
     "shell.execute_reply": "2021-10-01T22:08:00.772632Z",
     "shell.execute_reply.started": "2021-10-01T21:29:24.296962Z"
    },
    "papermill": {
     "duration": 64.175365,
     "end_time": "2021-10-01T22:08:00.773890",
     "exception": false,
     "start_time": "2021-10-01T22:06:56.598525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/pkg-wheels/einops-0.3.2-py3-none-any.whl\r\n",
      "Installing collected packages: einops\r\n",
      "Successfully installed einops-0.3.2\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "Processing /kaggle/input/pkg-wheels/faiss_gpu-1.6.3-cp37-cp37m-manylinux2010_x86_64.whl\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from faiss-gpu==1.6.3) (1.19.5)\r\n",
      "Installing collected packages: faiss-gpu\r\n",
      "Successfully installed faiss-gpu-1.6.3\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ../input/pkg-wheels/einops-0.3.2-py3-none-any.whl\n",
    "!pip install ../input/pkg-wheels/faiss_gpu-1.6.3-cp37-cp37m-manylinux2010_x86_64.whl\n",
    "!cp -r ../input/loftr-repo/ /kaggle/temp/\n",
    "sys.path.append('/kaggle/temp/loftr-repo/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99db2e3d",
   "metadata": {
    "papermill": {
     "duration": 0.044547,
     "end_time": "2021-10-01T22:08:00.863949",
     "exception": false,
     "start_time": "2021-10-01T22:08:00.819402",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70ce1518",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-10-01T22:08:00.963235Z",
     "iopub.status.busy": "2021-10-01T22:08:00.962265Z",
     "iopub.status.idle": "2021-10-01T22:08:10.301419Z",
     "shell.execute_reply": "2021-10-01T22:08:10.300803Z",
     "shell.execute_reply.started": "2021-10-01T21:30:24.844598Z"
    },
    "papermill": {
     "duration": 9.392901,
     "end_time": "2021-10-01T22:08:10.301576",
     "exception": false,
     "start_time": "2021-10-01T22:08:00.908675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "from scipy import spatial\n",
    "import pydegensac\n",
    "import copy\n",
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd \n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "import torch, cv2\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import warnings, math\n",
    "from torch.nn.parameter import Parameter\n",
    "import albumentations as A\n",
    "import timm, gc\n",
    "from sklearn.cluster import DBSCAN as dbscan\n",
    "from src.loftr import LoFTR, default_cfg\n",
    "import csv, shutil, glob, pickle, joblib\n",
    "# from memory_profiler import profile\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def seed_torch(seed=hyperparameters.seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_torch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e88a064",
   "metadata": {
    "papermill": {
     "duration": 0.04394,
     "end_time": "2021-10-01T22:08:10.389884",
     "exception": false,
     "start_time": "2021-10-01T22:08:10.345944",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9af7231d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:08:10.505418Z",
     "iopub.status.busy": "2021-10-01T22:08:10.504506Z",
     "iopub.status.idle": "2021-10-01T22:08:10.508401Z",
     "shell.execute_reply": "2021-10-01T22:08:10.507490Z",
     "shell.execute_reply.started": "2021-10-01T21:30:28.847026Z"
    },
    "papermill": {
     "duration": 0.074929,
     "end_time": "2021-10-01T22:08:10.508560",
     "exception": false,
     "start_time": "2021-10-01T22:08:10.433631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LandmarkDataset(Dataset):\n",
    "    def __init__(self,image_ids, mode = 'train', prob_type = 'retrieval'):\n",
    "        self.image_ids = image_ids\n",
    "        \n",
    "        if mode in ('train', 'index', 'test'):\n",
    "#             self.file_path = f'../input/landmark-recognition-2021/{mode}'\n",
    "            self.file_path = f'../input/landmark-{prob_type}-2021/{mode}'\n",
    "        elif mode == 'nolandmark':\n",
    "            self.file_path = f'../input/google-landmark-2021-validation/valid'\n",
    "        else:\n",
    "            raise\n",
    "            \n",
    "        self.transform = self._build_augmentation()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_     = self.image_ids[idx]\n",
    "        file_path = f'{self.file_path}/{file_[0]}/{file_[1]}/{file_[2]}/{file_}.jpg'\n",
    "        raw_image = cv2.imread(file_path)\n",
    "        img       = self.process_img(cv2.cvtColor(raw_image, cv2.COLOR_BGR2RGB))\n",
    "        img       = img.transpose(2,0,1)\n",
    "        return torch.from_numpy(img)\n",
    "    \n",
    "    def process_img(self, img):\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(image = img)['image']\n",
    "        img = img.astype(np.float32)\n",
    "        img = self.normalize(img)\n",
    "        return img\n",
    "        \n",
    "    def normalize(self, img):\n",
    "        mean = np.array([123.675, 116.28 , 103.53 ], dtype=np.float32)\n",
    "        std = np.array([58.395   , 57.120, 57.375   ], dtype=np.float32)\n",
    "        img = img.astype(np.float32)\n",
    "        img -= mean\n",
    "        img *= np.reciprocal(std, dtype=np.float32)\n",
    "        return img\n",
    "    \n",
    "    def _build_augmentation(self):\n",
    "        return  A.Compose([\n",
    "                A.SmallestMaxSize(512),\n",
    "                A.CenterCrop(height=448,width=448,p=1.)\n",
    "            ])\n",
    "    \n",
    "class GLRTRFDataset(Dataset):\n",
    "    def __init__(self, feat, pair_tuples):\n",
    "        self.feat = feat\n",
    "        self.pair_tuples = pair_tuples\n",
    "        return None\n",
    "    \n",
    "    def __getitem__(self, index:int):\n",
    "        sample      = self.pair_tuples[index]\n",
    "        query_emb   = self.feat[sample[0][0]:sample[0][0]+1]\n",
    "        gallary_emb = self.feat[sample[0][1]:sample[0][1]+1]\n",
    "        features    = np.expand_dims(np.array(sample[1]), 0)\n",
    "        label       = np.expand_dims(np.array(sample[2]), 0)\n",
    "        del sample\n",
    "        return query_emb, gallary_emb, features, label\n",
    "    \n",
    "    def __len__(self,):\n",
    "        return len(self.pair_tuples)\n",
    "    \n",
    "class GraphDataset(Dataset):\n",
    "\n",
    "    def __init__(self, feats=None, labels=None, weights=None, pair_tuples=None, k=50, top_neighbors=None):\n",
    "        self.feats = feats\n",
    "        self.labels = labels\n",
    "        self.weights = weights\n",
    "        self.pair_tuples = pair_tuples\n",
    "        self.k = k\n",
    "        self.top_neighbors = top_neighbors\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        i, j = self.pair_tuples[index]\n",
    "        feat = torch.FloatTensor(self.feats[i][j])\n",
    "\n",
    "        padding_i = [[0] * feat.shape[0]] * (self.k - len(self.top_neighbors[i]))\n",
    "        neighbor_feats_i = torch.FloatTensor([\n",
    "            self.feats[i][neighbor]\n",
    "            for neighbor in self.top_neighbors[i]\n",
    "        ] + padding_i)\n",
    "        padding_j = [[0] * feat.shape[0]] * (self.k - len(self.top_neighbors[j]))\n",
    "        neighbor_feats_j = torch.FloatTensor([\n",
    "            self.feats[j][neighbor]\n",
    "            for neighbor in self.top_neighbors[j]\n",
    "        ] + padding_j)\n",
    "        neighbor_feats = torch.cat([feat.unsqueeze(0), neighbor_feats_i, neighbor_feats_j], dim=0)\n",
    "\n",
    "        outputs = (feat, neighbor_feats)\n",
    "        if self.labels is not None:\n",
    "            outputs += (self.labels[i] == self.labels[j],)\n",
    "        if self.weights is not None:\n",
    "            outputs += (self.weights[i],)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pair_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30064693",
   "metadata": {
    "papermill": {
     "duration": 0.042116,
     "end_time": "2021-10-01T22:08:10.595268",
     "exception": false,
     "start_time": "2021-10-01T22:08:10.553152",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c91dd5e",
   "metadata": {
    "papermill": {
     "duration": 0.042601,
     "end_time": "2021-10-01T22:08:10.681968",
     "exception": false,
     "start_time": "2021-10-01T22:08:10.639367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Metric Learning Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa03fbbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:08:10.785646Z",
     "iopub.status.busy": "2021-10-01T22:08:10.784779Z",
     "iopub.status.idle": "2021-10-01T22:08:10.788959Z",
     "shell.execute_reply": "2021-10-01T22:08:10.788398Z",
     "shell.execute_reply.started": "2021-10-01T21:30:28.874861Z"
    },
    "papermill": {
     "duration": 0.064048,
     "end_time": "2021-10-01T22:08:10.789119",
     "exception": false,
     "start_time": "2021-10-01T22:08:10.725071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ArcMarginProduct2(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "\n",
    "    def forward(self, features):\n",
    "        cosine = F.linear(F.normalize(features), F.normalize(self.W))\n",
    "        return cosine\n",
    "    \n",
    "def l2_norm(input, axis = 1):\n",
    "    norm = torch.norm(input, 2, axis, True)\n",
    "    output = torch.div(input, norm)\n",
    "\n",
    "    return output\n",
    "    \n",
    "class CurricularFace(nn.Module):\n",
    "    r\"\"\"Implement of CurricularFace (https://arxiv.org/pdf/2004.00288.pdf):\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        device_id: the ID of GPU where the model will be trained by model parallel. \n",
    "                       if device_id=None, it will be trained on CPU without model parallel.\n",
    "        m: margin\n",
    "        s: scale of outputs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, m=0.4, s=45.0):\n",
    "        super(CurricularFace, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.kernel = Parameter(torch.Tensor(in_features, out_features))\n",
    "        self.register_buffer(\"t\", torch.zeros(1))\n",
    "\n",
    "    def forward(self, embbedings, label = None):\n",
    "        embbedings = l2_norm(embbedings, axis=1)\n",
    "        kernel_norm = l2_norm(self.kernel, axis=0)\n",
    "        cos_theta = torch.mm(embbedings, kernel_norm)\n",
    "        return cos_theta\n",
    "\n",
    "    \n",
    "class ArcMarginProduct_subcenter(nn.Module):\n",
    "    def __init__(self, in_features, out_features, k=3):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features*k, in_features))\n",
    "        self.reset_parameters()\n",
    "        self.k = k\n",
    "        self.out_features = out_features\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "    def forward(self, features):\n",
    "        cosine_all = F.linear(F.normalize(features), F.normalize(self.weight))\n",
    "        cosine_all = cosine_all.view(-1, self.out_features, self.k)\n",
    "        cosine, _ = torch.max(cosine_all, dim=2)\n",
    "        return cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57e37e6",
   "metadata": {
    "papermill": {
     "duration": 0.0427,
     "end_time": "2021-10-01T22:08:10.875367",
     "exception": false,
     "start_time": "2021-10-01T22:08:10.832667",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Pooling Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5382ff8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:08:10.985467Z",
     "iopub.status.busy": "2021-10-01T22:08:10.984302Z",
     "iopub.status.idle": "2021-10-01T22:08:10.987187Z",
     "shell.execute_reply": "2021-10-01T22:08:10.987681Z",
     "shell.execute_reply.started": "2021-10-01T21:30:29.148548Z"
    },
    "papermill": {
     "duration": 0.068461,
     "end_time": "2021-10-01T22:08:10.987852",
     "exception": false,
     "start_time": "2021-10-01T22:08:10.919391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gem(x, p=3, eps=1e-6):\n",
    "    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1.0 / p)[:,:,0,0]\n",
    "\n",
    "# class GeM(nn.Module):\n",
    "#     def __init__(self, p=3, eps=1e-6):\n",
    "#         super(GeM, self).__init__()\n",
    "#         self.p = Parameter(torch.ones(1) * p)\n",
    "#         self.eps = eps\n",
    "#     def forward(self, x):\n",
    "#         return gem(x, p=self.p, eps=self.eps)\n",
    "#     def __repr__(self):\n",
    "#         return (\n",
    "#             self.__class__.__name__\n",
    "#             + f\"(p={self.p.data.tolist()[0]:.4f}, eps={str(self.eps)})\"\n",
    "#         )\n",
    "\n",
    "class GridGeM(nn.Module):\n",
    "    def __init__(self, p = 3, eps = 1e-6):\n",
    "        super(GridGeM, self).__init__()\n",
    "        self.p = Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        f_a, f_b = torch.split(x,[W//2, W//2],dim=-1)\n",
    "        f_a, f_c = torch.split(f_a,[H//2, H//2],dim=-2)\n",
    "        f_b, f_d = torch.split(f_b,[H//2, H//2],dim=-2)\n",
    "\n",
    "        feats = torch.cat([gem(f_a, p=self.p, eps=self.eps),gem(f_b, p=self.p, eps=self.eps),gem(f_c, p=self.p, eps=self.eps),gem(f_d, p=self.p, eps=self.eps),], axis=1)\n",
    "        \n",
    "        return feats\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            self.__class__.__name__\n",
    "            + f\"(p={self.p.data.tolist()[0]:.4f}, eps={str(self.eps)})\"\n",
    "        )\n",
    "    \n",
    "class GridGeMCosdist(nn.Module):\n",
    "    def __init__(self, p = 3, eps = 1e-6):\n",
    "        super(GridGeMCosdist, self).__init__()\n",
    "        self.p = Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "        self.neck = nn.AdaptiveAvgPool2d( (1,1) )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        f_a, f_b = torch.split(x,[W//2, W//2],dim=-1)\n",
    "        f_a, f_c = torch.split(f_a,[H//2, H//2],dim=-2)\n",
    "        f_b, f_d = torch.split(f_b,[H//2, H//2],dim=-2)\n",
    "        f_a = f_a.mean(dim=-1).softmax(dim=-1)\n",
    "        f_b = f_b.mean(dim=-1).softmax(dim=-1)\n",
    "        f_c = f_c.mean(dim=-1).softmax(dim=-1)\n",
    "        f_d = f_d.mean(dim=-1).softmax(dim=-1)\n",
    "\n",
    "        feats = torch.stack([(1.0 - F.cosine_similarity(f_a, f_b, dim=-1)), (1.0 - F.cosine_similarity(f_a, f_c, dim=-1)),\n",
    "                           (1.0 - F.cosine_similarity(f_a, f_d, dim=-1)), (1.0 - F.cosine_similarity(f_b, f_c, dim=-1)),\n",
    "                          (1.0 - F.cosine_similarity(f_b, f_d, dim=-1)), (1.0 - F.cosine_similarity(f_c, f_d, dim=-1))]).view(-1,1)\n",
    "        \n",
    "        return feats\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            self.__class__.__name__\n",
    "            + f\"(p={self.p.data.tolist()[0]:.4f}, eps={str(self.eps)})\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5603df7",
   "metadata": {
    "papermill": {
     "duration": 0.045313,
     "end_time": "2021-10-01T22:08:11.077118",
     "exception": false,
     "start_time": "2021-10-01T22:08:11.031805",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Mix it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4e96c75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:08:11.181730Z",
     "iopub.status.busy": "2021-10-01T22:08:11.180613Z",
     "iopub.status.idle": "2021-10-01T22:08:11.183026Z",
     "shell.execute_reply": "2021-10-01T22:08:11.183598Z",
     "shell.execute_reply.started": "2021-10-01T21:30:29.173659Z"
    },
    "papermill": {
     "duration": 0.063334,
     "end_time": "2021-10-01T22:08:11.183779",
     "exception": false,
     "start_time": "2021-10-01T22:08:11.120445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LandmarkNet(nn.Module):\n",
    "    def __init__(self, out_feature, backbone='tf_efficientnet_b6_ns',  pretrained=True, pool_type = 'gem',metric ='arcface', sub_center = False):\n",
    "        super(LandmarkNet, self).__init__()\n",
    "        self.backbne_name = backbone\n",
    "        self.backbone = timm.create_model(backbone, pretrained=pretrained)\n",
    "        self.out_feature = out_feature\n",
    "\n",
    "        if \"efficientnet\" in backbone:\n",
    "            self.in_features = self.backbone.classifier.in_features\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif \"nfnet\" in backbone:\n",
    "            self.in_features = self.backbone.head.fc.in_features\n",
    "            self.backbone.head.fc = nn.Identity()\n",
    "            self.backbone.head.global_pool = nn.Identity()\n",
    "        elif 'resnet' in backbone:\n",
    "            self.in_features = self.backbone.fc.in_features\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "            self.backbone.fc = nn.Identity() \n",
    "            \n",
    "        self.out_feature = out_feature\n",
    "        \n",
    "        if pool_type == 'gem':\n",
    "            self.pooling =  GeM()\n",
    "        elif pool_type == 'gridgem':\n",
    "            self.pooling =  GridGeM()\n",
    "            self.in_features *= 4\n",
    "        elif pool_type == 'gridgemcosdist':\n",
    "            self.pooling =  GridGeMCosdist()\n",
    "            self.in_features *= 6\n",
    "        else:\n",
    "            self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "            \n",
    "        self.neck = nn.Sequential(\n",
    "                nn.Linear(self.in_features, 512, bias=True),\n",
    "                nn.BatchNorm1d(512),\n",
    "                torch.nn.PReLU()\n",
    "            )\n",
    "        if metric == 'arcface':\n",
    "            if sub_center:\n",
    "                self.final = ArcMarginProduct_subcenter(512, self.out_feature)\n",
    "            else:\n",
    "                self.final = ArcMarginProduct2(512, self.out_feature)\n",
    "        elif metric == 'curricular':\n",
    "            self.final = CurricularFace(512, self.out_feature)\n",
    "\n",
    "    def forward(self, x, get_embeddings = True):\n",
    "        if \"efficientnet\" in self.backbne_name or \"nfnet\" in self.backbne_name:\n",
    "            batch_size = x.shape[0]\n",
    "            features = self.backbone(x)\n",
    "            features0 = self.pooling(features).view(batch_size, -1)\n",
    "            features = self.neck(features0)\n",
    "            features0 = F.normalize(features0)\n",
    "            features = F.normalize(features)\n",
    "\n",
    "        if not get_embeddings:\n",
    "            return self.final(features)\n",
    "        return features0 , features, self.final(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa22402",
   "metadata": {
    "papermill": {
     "duration": 0.045887,
     "end_time": "2021-10-01T22:08:11.276990",
     "exception": false,
     "start_time": "2021-10-01T22:08:11.231103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "014021a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:08:11.393971Z",
     "iopub.status.busy": "2021-10-01T22:08:11.375981Z",
     "iopub.status.idle": "2021-10-01T22:08:11.396790Z",
     "shell.execute_reply": "2021-10-01T22:08:11.397313Z",
     "shell.execute_reply.started": "2021-10-01T21:30:29.190985Z"
    },
    "papermill": {
     "duration": 0.074105,
     "end_time": "2021-10-01T22:08:11.397475",
     "exception": false,
     "start_time": "2021-10-01T22:08:11.323370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, state_size=200):\n",
    "        super(FFN, self).__init__()\n",
    "        self.state_size = state_size\n",
    "\n",
    "        self.lr1 = nn.Linear(state_size, state_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lr2 = nn.Linear(state_size, state_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.lr1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lr2(x)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.scale = nn.Parameter(torch.ones(1))\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(\n",
    "            0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.scale * self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def future_mask(seq_length):\n",
    "    future_mask = np.triu(np.ones((seq_length, seq_length)), k=1).astype('bool')\n",
    "    return torch.from_numpy(future_mask)\n",
    "\n",
    "class GLRTRFModel(nn.Module):\n",
    "    def __init__(self, embed_dim=128, dropout_rate=0.2):\n",
    "        super(GLRTRFModel, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.cat = nn.Sequential(\n",
    "            nn.Linear(embed_dim*2, embed_dim),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "        ) \n",
    "        self.cat1 = nn.Sequential(\n",
    "            nn.Linear(embed_dim*2, embed_dim),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "        ) \n",
    "        \n",
    "        self.transformer = nn.Transformer(nhead=8, d_model = embed_dim, num_encoder_layers= N_LAYER, num_decoder_layers= N_LAYER, dropout = dropout_rate)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer_normal = nn.LayerNorm(embed_dim) \n",
    "\n",
    "        self.ffn = FFN(embed_dim)\n",
    "        \n",
    "        self.mlp1 = nn.Linear(N_FEATURE, 1024)\n",
    "        self.mlp2 = nn.ReLU()\n",
    "        self.mlp3 = nn.Linear(1024, 512)\n",
    "        self.mlp4 = nn.ReLU()\n",
    "        self.mlp5 = nn.Linear(512, embed_dim)\n",
    "        self.pred = nn.Linear(embed_dim*2, 1)\n",
    "    \n",
    "    def forward(self, query_embedding, gallary_embedding, features):\n",
    "        device = query_embedding.device \n",
    "        \n",
    "        x1 = self.mlp1(features)\n",
    "        x1 = self.mlp2(x1)\n",
    "        x1 = self.mlp3(x1)\n",
    "        x1 = self.mlp4(x1)\n",
    "        x1 = self.mlp5(x1)\n",
    "\n",
    "        x = torch.cat([query_embedding,x1], axis=-1)\n",
    "        x = self.cat(x)\n",
    "        e = torch.cat([gallary_embedding, x1], axis=-1)\n",
    "        e = self.cat1(e)\n",
    "        \n",
    "        x = x.permute(1, 0, 2)\n",
    "        e = e.permute(1, 0, 2)\n",
    "        \n",
    "        att_mask = future_mask(x.size(0)).to(device)\n",
    "        att_output = self.transformer( x,e, src_mask=att_mask, tgt_mask=att_mask, memory_mask = att_mask)\n",
    "        att_output = self.layer_normal(att_output+e)\n",
    "        att_output = att_output.permute(1, 0, 2)\n",
    "        x = self.ffn(att_output)\n",
    "        x = self.layer_normal(x + att_output)\n",
    "        x = torch.cat([x,x1], axis = -1)\n",
    "        x = self.pred(x)\n",
    "        return x.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9154337b",
   "metadata": {
    "papermill": {
     "duration": 0.048225,
     "end_time": "2021-10-01T22:08:11.499881",
     "exception": false,
     "start_time": "2021-10-01T22:08:11.451656",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c3ba6d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:08:11.616380Z",
     "iopub.status.busy": "2021-10-01T22:08:11.612414Z",
     "iopub.status.idle": "2021-10-01T22:08:11.619411Z",
     "shell.execute_reply": "2021-10-01T22:08:11.618899Z",
     "shell.execute_reply.started": "2021-10-01T21:30:29.216423Z"
    },
    "papermill": {
     "duration": 0.071751,
     "end_time": "2021-10-01T22:08:11.619630",
     "exception": false,
     "start_time": "2021-10-01T22:08:11.547879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout=0.6, alpha=0.2, concat=True):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.empty(size=(2 * out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, h):\n",
    "        Wh = h @ self.W  # h.shape: (B, N, in_features), Wh.shape: (B, N, out_features)\n",
    "        a_input = self._prepare_attentional_mechanism_input(Wh)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(3))\n",
    "\n",
    "        attention = F.softmax(e, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.bmm(attention, Wh)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        B, N, D = Wh.shape\n",
    "\n",
    "        Wh_repeated_in_chunks = Wh.repeat_interleave(N, dim=1)\n",
    "        Wh_repeated_alternating = Wh.repeat(1, N, 1)\n",
    "\n",
    "        all_combinations_matrix = torch.cat([Wh_repeated_in_chunks, Wh_repeated_alternating], dim=2)\n",
    "        return all_combinations_matrix.view(-1, N, N, 2 * D)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class GATPairClassifier(nn.Module):\n",
    "    def __init__(self, nfeat, nhid=8, nclass=1, dropout=0.6, alpha=0.2, nheads=8, pooling='first'):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.pooling = pooling\n",
    "\n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_att = GraphAttentionLayer(nhid * nheads, nhid, dropout=dropout, alpha=alpha, concat=False)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(nfeat + nhid, nhid),\n",
    "            nn.PReLU(),\n",
    "            nn.BatchNorm1d(nhid),\n",
    "            nn.Linear(nhid, nclass),\n",
    "        )\n",
    "\n",
    "    def forward_gat(self, x):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x) for att in self.attentions], dim=2)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x))\n",
    "        if self.pooling == 'first':\n",
    "            return x[:, 0]\n",
    "        elif self.pooling == 'mean':\n",
    "            return x.mean(dim=1)\n",
    "\n",
    "    def forward(self, feats, neighbor_feats):\n",
    "        gat_feats = self.forward_gat(neighbor_feats)\n",
    "        cat_feats = torch.cat([feats, gat_feats], dim=1)\n",
    "        return self.classifier(cat_feats).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92b4ff0",
   "metadata": {
    "papermill": {
     "duration": 0.043164,
     "end_time": "2021-10-01T22:08:11.706122",
     "exception": false,
     "start_time": "2021-10-01T22:08:11.662958",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0280e725",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:08:11.800496Z",
     "iopub.status.busy": "2021-10-01T22:08:11.799436Z",
     "iopub.status.idle": "2021-10-01T22:08:11.802458Z",
     "shell.execute_reply": "2021-10-01T22:08:11.801936Z",
     "shell.execute_reply.started": "2021-10-01T21:30:29.239311Z"
    },
    "papermill": {
     "duration": 0.053054,
     "end_time": "2021-10-01T22:08:11.802585",
     "exception": false,
     "start_time": "2021-10-01T22:08:11.749531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_total_score(global_score, local_score, transformer_score, gcn_score, lgb_score, weights = [1]*5):\n",
    "    classifier_score = (transformers_score * weights[2] + gcn_score * weights[3] + lgb_score * weights[4])\n",
    "    score = (global_score * classifier_score * weights[0]) + ((local_score/MAX_INLIER_SCORE) * classifier_score * weights[0])\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b0aefc",
   "metadata": {
    "papermill": {
     "duration": 0.043475,
     "end_time": "2021-10-01T22:08:11.890108",
     "exception": false,
     "start_time": "2021-10-01T22:08:11.846633",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c2cd2ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:08:11.985480Z",
     "iopub.status.busy": "2021-10-01T22:08:11.984306Z",
     "iopub.status.idle": "2021-10-01T22:08:11.986836Z",
     "shell.execute_reply": "2021-10-01T22:08:11.987451Z",
     "shell.execute_reply.started": "2021-10-01T21:30:29.250717Z"
    },
    "papermill": {
     "duration": 0.053922,
     "end_time": "2021-10-01T22:08:11.987627",
     "exception": false,
     "start_time": "2021-10-01T22:08:11.933705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_labelmap(TRAIN_LABELMAP_PATH):\n",
    "    with open(TRAIN_LABELMAP_PATH, mode='r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        labelmap = {row['id']: row['landmark_id'] for row in csv_reader}\n",
    "    gc.collect()\n",
    "    return labelmap\n",
    "\n",
    "def memory_cleanup():\n",
    "    \"\"\"\n",
    "    Cleans up GPU memory\n",
    "    https://github.com/huggingface/transformers/issues/1742\n",
    "    \"\"\"\n",
    "    for obj in gc.get_objects():\n",
    "        if torch.is_tensor(obj):\n",
    "            del obj\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffcc357",
   "metadata": {
    "papermill": {
     "duration": 0.043844,
     "end_time": "2021-10-01T22:08:12.075373",
     "exception": false,
     "start_time": "2021-10-01T22:08:12.031529",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d131b0b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:08:12.177276Z",
     "iopub.status.busy": "2021-10-01T22:08:12.176098Z",
     "iopub.status.idle": "2021-10-01T22:08:12.178551Z",
     "shell.execute_reply": "2021-10-01T22:08:12.179144Z",
     "shell.execute_reply.started": "2021-10-01T21:30:29.259084Z"
    },
    "papermill": {
     "duration": 0.060516,
     "end_time": "2021-10-01T22:08:12.179312",
     "exception": false,
     "start_time": "2021-10-01T22:08:12.118796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_submission_csv(DATASET_DIR, test_ids = None, predictions=None, mode = 'recognition',df_pair = None, submit_fname = 'submission.csv'):\n",
    "    \"\"\"Saves optional `predictions` as submission.csv.\n",
    "\n",
    "      The csv has columns {id, landmarks}. The landmarks column is a string\n",
    "      containing the label and score for the id, separated by a ws delimeter.\n",
    "\n",
    "      If `predictions` is `None` (default), submission.csv is copied from\n",
    "      sample_submission.csv in `IMAGE_DIR`.\n",
    "\n",
    "      Args:\n",
    "        predictions: Optional dict of image ids to dicts with keys {class, score}.\n",
    "      \"\"\"\n",
    "    print('Submitting ..... ')\n",
    "    if mode == 'retrieval':\n",
    "        predicitions = defaultdict(list)\n",
    "        for ix, i in enumerate(predictions):\n",
    "            tmp = []\n",
    "            for j in range(len(i)):\n",
    "                tmp.append(i[j][0])\n",
    "            predicitions[test_ids[ix]] = tmp\n",
    "    elif mode == 'recognition':\n",
    "        predicitions = defaultdict(lambda: defaultdict())\n",
    "        for ix, i in enumerate(predictions):\n",
    "            tmp = {}\n",
    "#             for j in range(len(i)):\n",
    "            tmp['class'] = i[0][1]\n",
    "            tmp['score'] = i[0][2]\n",
    "            predicitions[test_ids[ix]] = tmp\n",
    "            \n",
    "    gc.collect()\n",
    "    if predictions is None:\n",
    "    # Dummy submission!\n",
    "        shutil.copyfile(\n",
    "            os.path.join(DATASET_DIR, 'sample_submission.csv'), 'submission.csv')\n",
    "        return\n",
    "    \n",
    "    if mode == 'recognition':\n",
    "        with open(submit_fname, 'w') as submission_csv:\n",
    "            csv_writer = csv.DictWriter(submission_csv, fieldnames=['id', 'landmarks'])\n",
    "            csv_writer.writeheader()\n",
    "            for ix,(image_id, prediction) in enumerate(predicitions.items()):\n",
    "                label = prediction['class']\n",
    "                score = prediction['score']\n",
    "                csv_writer.writerow({'id': image_id, 'landmarks': f'{label} {score}'})\n",
    "        gc.collect()\n",
    "        return submit_fname\n",
    "    else:\n",
    "        with open(submit_fname, 'w') as submission_csv:\n",
    "            csv_writer = csv.DictWriter(submission_csv, fieldnames=['id', 'images'])\n",
    "            csv_writer.writeheader()\n",
    "            for image_id, prediction in predicitions.items():\n",
    "                image_ids = prediction\n",
    "                csv_writer.writerow({'id': image_id, 'images': ' '.join(image_ids)})\n",
    "        gc.collect()\n",
    "        return submit_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fac19677",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:08:12.270094Z",
     "iopub.status.busy": "2021-10-01T22:08:12.269034Z",
     "iopub.status.idle": "2021-10-01T22:08:12.275956Z",
     "shell.execute_reply": "2021-10-01T22:08:12.276588Z",
     "shell.execute_reply.started": "2021-10-01T21:30:29.276161Z"
    },
    "papermill": {
     "duration": 0.053832,
     "end_time": "2021-10-01T22:08:12.276745",
     "exception": false,
     "start_time": "2021-10-01T22:08:12.222913",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def save_submission_csv(DATASET_DIR,df_pair, test_ids = None, predictions=None, mode = 'recognition', submit_fname = 'submission.csv'):\n",
    "#     \"\"\"Saves optional `predictions` as submission.csv.\n",
    "\n",
    "#       The csv has columns {id, landmarks}. The landmarks column is a string\n",
    "#       containing the label and score for the id, separated by a ws delimeter.\n",
    "\n",
    "#       If `predictions` is `None` (default), submission.csv is copied from\n",
    "#       sample_submission.csv in `IMAGE_DIR`.\n",
    "\n",
    "#       Args:\n",
    "#         predictions: Optional dict of image ids to dicts with keys {class, score}.\n",
    "#       \"\"\"\n",
    "#     print('Submitting ..... ')\n",
    "#     if mode == 'retrieval':\n",
    "#         predicitions = defaultdict(lambda: defaultdict())\n",
    "#         for ix, i in enumerate(predictions):\n",
    "#             tmp = {}\n",
    "#             for j in range(len(i)):\n",
    "#                 s = df_pair[(df_pair.id == test_ids[ix]) & (df_pair.id_target == i[j][0])]['pred'].values\n",
    "#                 tmp['target_id'] = i[j][0]\n",
    "#                 tmp['score'] = i[j][2]*s\n",
    "#             predicitions[test_ids[ix]] = tmp\n",
    "#         for (image_id, pr) in predicitions.items():\n",
    "#             predicitions[image_id] = sorted(pr, key = lambda x: x['score'])\n",
    "        \n",
    "#     elif mode == 'recognition':\n",
    "#         predicitions = defaultdict(lambda: defaultdict())\n",
    "#         for ix, i in enumerate(predictions):\n",
    "#             tmp = {}\n",
    "#             for j in range(len(i)):\n",
    "#                 s = df_pair[(df_pair.id == test_ids[ix]) & (df_pair.id_target == i[j][0])]['pred'].values\n",
    "#                 tmp['class'] = i[j][1]\n",
    "#                 tmp['score'] = i[j][2]*s\n",
    "#             predicitions[test_ids[ix]] = tmp\n",
    "\n",
    "#         for image_id, pr in predicitions.items():\n",
    "#             predicitions[image_id] = sorted(pr, key = lambda x: x['score'])\n",
    "        \n",
    "            \n",
    "#     gc.collect()\n",
    "#     if predictions is None:\n",
    "#     # Dummy submission!\n",
    "#         shutil.copyfile(\n",
    "#             os.path.join(DATASET_DIR, 'sample_submission.csv'), 'submission.csv')\n",
    "#         return\n",
    "    \n",
    "#     if mode == 'recognition':\n",
    "#         with open(submit_fname, 'w') as submission_csv:\n",
    "#             csv_writer = csv.DictWriter(submission_csv, fieldnames=['id', 'landmarks'])\n",
    "#             csv_writer.writeheader()\n",
    "#             for ix,(image_id, prediction) in enumerate(predicitions.items()):\n",
    "#                 label = prediction['class']\n",
    "#                 score = prediction['score']\n",
    "#                 csv_writer.writerow({'id': image_id, 'landmarks': f'{label} {score}'})\n",
    "#         gc.collect()\n",
    "#         return submit_fname\n",
    "#     else:\n",
    "#         with open(submit_fname, 'w') as submission_csv:\n",
    "#             csv_writer = csv.DictWriter(submission_csv, fieldnames=['id', 'images'])\n",
    "#             csv_writer.writeheader()\n",
    "#             for image_id, prediction in predicitions.items():\n",
    "#                 image_ids = prediction\n",
    "#                 csv_writer.writerow({'id': image_id, 'images': ' '.join(image_ids)})\n",
    "#         gc.collect()\n",
    "#         return submit_fname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4e14e7",
   "metadata": {
    "papermill": {
     "duration": 0.043946,
     "end_time": "2021-10-01T22:08:12.364620",
     "exception": false,
     "start_time": "2021-10-01T22:08:12.320674",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Global Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a551ed9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:08:12.462771Z",
     "iopub.status.busy": "2021-10-01T22:08:12.461726Z",
     "iopub.status.idle": "2021-10-01T22:08:12.465013Z",
     "shell.execute_reply": "2021-10-01T22:08:12.464340Z",
     "shell.execute_reply.started": "2021-10-01T21:30:29.286077Z"
    },
    "papermill": {
     "duration": 0.056698,
     "end_time": "2021-10-01T22:08:12.465176",
     "exception": false,
     "start_time": "2021-10-01T22:08:12.408478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_global_features(df, mode, prob_type, return_probs = False):\n",
    "    dataset     = LandmarkDataset(df['id'].values, mode = mode, prob_type = prob_type)\n",
    "    dataloader  = DataLoader(dataset, batch_size = hyperparameters.batch_size, shuffle=False, num_workers=hyperparameters.num_workers)\n",
    "    feats = []\n",
    "    with torch.no_grad():\n",
    "        for img in tqdm(dataloader):\n",
    "            img  = img.cuda()\n",
    "            feat = []\n",
    "            for model in models:\n",
    "                _,feat_m,_  = model(img)\n",
    "                feat.append(feat_m)\n",
    "            feat = torch.cat(feat,dim=1) \n",
    "#             feat = torch.sum(torch.stack([feat]), dim = 0)\n",
    "            \n",
    "#             if return_probs:\n",
    "#                 probs_m = (lo_1 + lo_2)/2\n",
    "#                 save_pickle(probs_m,f'{mode}_probs.pkl')\n",
    "                \n",
    "            feats.append(feat.detach().cpu())\n",
    "        feats = torch.cat(feats)\n",
    "        feats = feats.cuda()\n",
    "        feat  = F.normalize(feat) \n",
    "#         save_pickle(feats,f'{mode}_embedding')\n",
    "    del feat, img, dataset, dataloader\n",
    "    gc.collect()\n",
    "    if return_probs:\n",
    "        return df['id'].values, feats, prob_m\n",
    "    return df['id'].values,feats.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f1e477",
   "metadata": {
    "papermill": {
     "duration": 0.043616,
     "end_time": "2021-10-01T22:08:12.552186",
     "exception": false,
     "start_time": "2021-10-01T22:08:12.508570",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LoFTR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c8f9612",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:08:12.661184Z",
     "iopub.status.busy": "2021-10-01T22:08:12.648638Z",
     "iopub.status.idle": "2021-10-01T22:08:12.664171Z",
     "shell.execute_reply": "2021-10-01T22:08:12.663622Z",
     "shell.execute_reply.started": "2021-10-01T21:30:29.298300Z"
    },
    "papermill": {
     "duration": 0.067617,
     "end_time": "2021-10-01T22:08:12.664323",
     "exception": false,
     "start_time": "2021-10-01T22:08:12.596706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_loftr(matcher, image_0, train_images):\n",
    "    img0 = []\n",
    "    img1 = []\n",
    "    img0_scales = []\n",
    "    img1_scales = []\n",
    "    for image_1 in train_images:\n",
    "        img0_raw = cv2.imread(image_0, cv2.IMREAD_GRAYSCALE)\n",
    "        img1_raw = cv2.imread(image_1, cv2.IMREAD_GRAYSCALE)\n",
    "        img0_scales.append((img0_raw.shape[1]/640, img0_raw.shape[0]/480))\n",
    "        img1_scales.append((img1_raw.shape[1]/640, img1_raw.shape[0]/480))\n",
    "        img0.append(cv2.resize(img0_raw, (640, 480)))\n",
    "        img1.append(cv2.resize(img1_raw, (640, 480)))\n",
    "        \n",
    "    del img0_raw, img1_raw\n",
    "        \n",
    "    img0 = torch.from_numpy(np.array(img0)[:,None,...]).cuda() / 255.\n",
    "    img1 = torch.from_numpy(np.array(img1)[:,None,...]).cuda() / 255.\n",
    "    batch = {'image0': img0, 'image1': img1}\n",
    "    del img0 , img1\n",
    "    with torch.no_grad():\n",
    "        matcher(batch)\n",
    "    del batch['image0'], batch['image1']\n",
    "    batch['img0_scales'] = img0_scales\n",
    "    batch['img1_scales'] = img1_scales\n",
    "    gc.collect()\n",
    "    del matcher\n",
    "    return batch\n",
    "    \n",
    "def rescore_and_rerank(test_image_dir, train_image_dir,\n",
    "                                      test_image_id, train_ids_labels_and_scores, batch_size = 16, ignore_global_score=False, do_sort=True,\n",
    "                                      loftr_model=None, return_num_inliers=False):\n",
    "    \n",
    "    test_image_path = f'{test_image_dir}/{test_image_id[0]}/{test_image_id[1]}/{test_image_id[2]}/{test_image_id}.jpg'\n",
    "    test_image_dict = {}\n",
    "\n",
    "    ransac_inliers = []\n",
    "    \n",
    "    train_images_path = []\n",
    "    \n",
    "    for i in range(len(train_ids_labels_and_scores)):\n",
    "        train_image_id, label, global_score = train_ids_labels_and_scores[i]\n",
    "        train_images_path.append(f'{train_image_dir}/{train_image_id[0]}/{train_image_id[1]}/{train_image_id[2]}/{train_image_id}.jpg')\n",
    "        \n",
    "    for batch_i in range(0,len(train_images_path), batch_size):\n",
    "        pred = generate_loftr(loftr_model,test_image_path,train_images_path[batch_i:batch_i+batch_size] )\n",
    "        test_scales = pred['img0_scales'][0]\n",
    "        test_keypoints = copy.deepcopy(pred['mkpts0_f']).cpu().numpy()*test_scales[0]\n",
    "        m_bids = pred['m_bids'].cpu().numpy()\n",
    "        del test_scales\n",
    "        up = batch_size if batch_i + batch_size < len(train_images_path) else len(train_images_path) - batch_i\n",
    "        for i in range(up):\n",
    "            train_image_id, label, global_score = train_ids_labels_and_scores[batch_i + i]\n",
    "            train_scales = pred['img1_scales'][i]\n",
    "            train_keypoints = copy.deepcopy(pred['mkpts1_f'][m_bids == i]).cpu().numpy()*train_scales\n",
    "            num_inliers = compute_num_inliers(test_keypoints[m_bids == i], None, train_keypoints, None, do_kdtree=False)\n",
    "\n",
    "            total_score = global_score + num_inliers/MAX_INLIER_SCORE\n",
    "            train_ids_labels_and_scores[batch_i + i] = (train_image_id, label, total_score)\n",
    "            ransac_inliers.append((train_image_id, num_inliers))\n",
    "        del pred, test_keypoints, train_keypoints, train_scales\n",
    "        \n",
    "    if do_sort:\n",
    "        train_ids_labels_and_scores.sort(key=lambda x: x[2], reverse=True)\n",
    "    gc.collect()   \n",
    "    if return_num_inliers:\n",
    "        return ransac_inliers\n",
    "    else:\n",
    "        return train_ids_labels_and_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c75ab5",
   "metadata": {
    "papermill": {
     "duration": 0.043769,
     "end_time": "2021-10-01T22:08:12.752505",
     "exception": false,
     "start_time": "2021-10-01T22:08:12.708736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d35501e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:08:12.859932Z",
     "iopub.status.busy": "2021-10-01T22:08:12.856339Z",
     "iopub.status.idle": "2021-10-01T22:08:12.862938Z",
     "shell.execute_reply": "2021-10-01T22:08:12.862441Z",
     "shell.execute_reply.started": "2021-10-01T21:30:29.318503Z"
    },
    "papermill": {
     "duration": 0.065684,
     "end_time": "2021-10-01T22:08:12.863092",
     "exception": false,
     "start_time": "2021-10-01T22:08:12.797408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_gcn(df_train,df_test, gcn_rerank = 25, topk = 15):\n",
    "    \n",
    "    indexes_img = load_pickle('ind_train_test_sim.npy')\n",
    "    similarities_img = load_pickle('val_train_test_sim.npy')\n",
    "    row = indexes_img.ravel()\n",
    "    col = np.arange(len(indexes_img)).repeat(topk)\n",
    "    data = similarities_img.ravel()\n",
    "    \n",
    "    simmat_img = {(i, j): d for i, j, d in zip(col, row, data)}\n",
    "    \n",
    "    top_neighbors = defaultdict(list)\n",
    "    feats = defaultdict(lambda: defaultdict())\n",
    "    pair_tuples = []\n",
    "    for i in tqdm(range(len(df_test))):\n",
    "        right_indexes = set(indexes_img[i, :gcn_rerank].tolist())\n",
    "\n",
    "        right_indexes = list(right_indexes)\n",
    "        scores = {}\n",
    "        for j in right_indexes:\n",
    "            pair_tuples.append((i, j))\n",
    "\n",
    "            sim_img = simmat_img.get((i, j), 0)\n",
    "            if sim_img == 0:\n",
    "                print('.',end = '')\n",
    "                continue\n",
    "\n",
    "            feats[i][j] = [\n",
    "                sim_img,\n",
    "            ]\n",
    "            scores[j] = sim_img\n",
    "\n",
    "        top_neighbors[i] = sorted(right_indexes, key=lambda x: scores[x], reverse=True)[:gcn_rerank]\n",
    "\n",
    "    dataset = GraphDataset(\n",
    "        feats=feats,\n",
    "        pair_tuples=pair_tuples,\n",
    "        k=15,\n",
    "        top_neighbors=top_neighbors,\n",
    "    )\n",
    "    loader = DataLoader(dataset, batch_size=2 ** 6, shuffle=False, drop_last=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    gat = GATPairClassifier(nfeat=1, nhid=16, dropout=0.0, nheads=16, pooling='first')\n",
    "    gat.to('cuda').eval()\n",
    "    gat.load_state_dict(torch.load('../input/model/glrtrf_model_last.pt'))\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    preds = []\n",
    "    for feats, neighbor_feats in tqdm(loader, desc='predict', leave=False):\n",
    "        feats = feats.to('cuda', non_blocking=True)\n",
    "        neighbor_feats = neighbor_feats.to('cuda', non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            pred = gat(feats, neighbor_feats).sigmoid().detach().cpu().numpy().tolist()\n",
    "            preds.extend(pred)\n",
    "    \n",
    "    \n",
    "    df_pair = pd.DataFrame()\n",
    "    df_pair['i'] = col\n",
    "    df_pair['j'] = row\n",
    "\n",
    "    df_pair['id'] = df_test['id'].values[df_pair['i'].values]\n",
    "    df_pair['id_target'] = df_train['id'].values[df_pair['j'].values]\n",
    "    df_pair['pred'] = preds\n",
    "    del gat, loader, dataset, top_neighbors, scores, feats, right_indexes, pair_tuples, col, row, data, indexes_img, similarities_img\n",
    "    return df_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a959c88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:08:12.957830Z",
     "iopub.status.busy": "2021-10-01T22:08:12.956816Z",
     "iopub.status.idle": "2021-10-01T22:08:12.959783Z",
     "shell.execute_reply": "2021-10-01T22:08:12.959282Z",
     "shell.execute_reply.started": "2021-10-01T21:30:29.336795Z"
    },
    "papermill": {
     "duration": 0.053507,
     "end_time": "2021-10-01T22:08:12.959939",
     "exception": false,
     "start_time": "2021-10-01T22:08:12.906432",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine(d1, d2, i1,i2):\n",
    "    for i in range(d1.shape[0]):\n",
    "        for j in range(d1.shape[1]):\n",
    "            for k in range(d2.shape[1]):\n",
    "                if i2[i,k] == i1[i, j]:\n",
    "                    if d1[i, j] < 0:\n",
    "                        d1[i, j] *= (1 - d2[i,k])\n",
    "                    else:\n",
    "                        d1[i, j] *= d2[i,k]\n",
    "    \n",
    "                        \n",
    "    return d1, i1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "617f1578",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:08:13.068955Z",
     "iopub.status.busy": "2021-10-01T22:08:13.061909Z",
     "iopub.status.idle": "2021-10-01T22:08:13.073183Z",
     "shell.execute_reply": "2021-10-01T22:08:13.072590Z",
     "shell.execute_reply.started": "2021-10-01T21:30:29.347002Z"
    },
    "papermill": {
     "duration": 0.068647,
     "end_time": "2021-10-01T22:08:13.073358",
     "exception": false,
     "start_time": "2021-10-01T22:08:13.004711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_expansion(feats, sims, topk_idx, alpha=0.5, k=2):\n",
    "    weights = np.expand_dims(sims[:, :k] ** alpha, axis=-1).astype(np.float32)\n",
    "    feats   = (feats[topk_idx[:, :k]] * weights).sum(axis=1)\n",
    "    feats  /= np.linalg.norm(feats, 2, axis=1, keepdims=True)\n",
    "    return feats\n",
    "\n",
    "def do_retrieval(labelmap, train_ids, train_embeddings,\n",
    "                 test_embeddings, num_to_rerank,df_pair = None, val_x = None, val_y = None, do_dba=False,\n",
    "                 gallery_set='index', method = 'cossim',return_vals = False,lgb = False,save = False, name ='simple'):\n",
    "    train_ids_labels_and_scores = [None] * test_embeddings.shape[0]\n",
    "    \n",
    "    if method == 'faiss':\n",
    "        res = faiss.StandardGpuResources()\n",
    "        faiss_index = faiss.IndexFlatIP(train_embeddings.shape[1])\n",
    "        faiss_index = faiss.index_cpu_to_gpu(res, 0, faiss_index)\n",
    "        faiss_index.add(train_embeddings)\n",
    "        D, I = faiss_index.search(test_embeddings, num_to_rerank)  # actual search\n",
    "        \n",
    "    elif method == 'cossim':\n",
    "        if val_x is None:\n",
    "            D, I = get_topk_cossim(test_embeddings, train_embeddings, batchsize = 64, k=num_to_rerank, device='cuda:0',verbose=True,save = save, name = name)\n",
    "        else:\n",
    "            D, I = get_topk_cossim_sub(test_embeddings, train_embeddings,val_x, batchsize = 64, k=num_to_rerank, device='cuda:0',verbose=True,save = save, name = name)\n",
    "        if val_y is not None:\n",
    "            D -= 1* val_y\n",
    "    \n",
    "    for test_index in range(test_embeddings.shape[0]):\n",
    "        train_ids_labels_and_scores[test_index] = [\n",
    "          (train_ids[train_index], labelmap[train_ids[train_index]], distance)\n",
    "          for train_index, distance in zip(I[test_index], D[test_index])\n",
    "        ]\n",
    "    del I\n",
    "    gc.collect()\n",
    "    if return_vals:\n",
    "        return train_ids_labels_and_scores, D\n",
    "    del D\n",
    "    return train_ids_labels_and_scores\n",
    "\n",
    "def get_nolandmark_by_dbscan(test_ids, test_embeddings, nolandmark_ids, nolandmark_embeddings):\n",
    "  # dbscan\n",
    "    features = np.vstack([test_embeddings, nolandmark_embeddings])\n",
    "    clusters = dbscan(eps=0.85, n_jobs=-1, min_samples=1).fit_predict(features)\n",
    "    clusters_np = np.c_[np.r_[test_ids, nolandmark_ids], clusters]\n",
    "    clusters_df = pd.DataFrame(data=clusters_np, columns=[ID, 'clusters'])\n",
    "    clusters_df['is_nolandmark'] = [0]*len(test_ids) + [1]*len(nolandmark_ids)\n",
    "    clusters_gb = clusters_df.groupby('clusters')['is_nolandmark'].agg(['count', 'sum']).reset_index()\n",
    "    clusters_gb.columns = ['clusters', 'clusters_num', 'nolandmark_num']\n",
    "    clusters_gb['nolandmark_rate'] = clusters_gb['nolandmark_num'] / clusters_gb['clusters_num']\n",
    "\n",
    "    test_clusters = clusters_df[0: len(test_ids)]\n",
    "    test_clusters = test_clusters.merge(clusters_gb, on='clusters', how='left')\n",
    "    gc.collect()\n",
    "    return test_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7167749",
   "metadata": {
    "papermill": {
     "duration": 0.04293,
     "end_time": "2021-10-01T22:08:13.161153",
     "exception": false,
     "start_time": "2021-10-01T22:08:13.118223",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CosSim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f03d3b06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:08:13.264210Z",
     "iopub.status.busy": "2021-10-01T22:08:13.263098Z",
     "iopub.status.idle": "2021-10-01T22:08:13.265458Z",
     "shell.execute_reply": "2021-10-01T22:08:13.266071Z",
     "shell.execute_reply.started": "2021-10-01T21:30:29.368045Z"
    },
    "papermill": {
     "duration": 0.062006,
     "end_time": "2021-10-01T22:08:13.266227",
     "exception": false,
     "start_time": "2021-10-01T22:08:13.204221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cos_similarity_matrix(a, b, eps=1e-8):\n",
    "    sim_mt = torch.mm(a, b.transpose(0, 1))\n",
    "    return sim_mt\n",
    "\n",
    "def get_topk_cossim(test_emb, tr_emb, batchsize = 64, k=10, device='cuda:0',verbose=True, save = False, name = 'simple'):\n",
    "    tr_emb = torch.tensor(tr_emb, dtype = torch.float32, device=torch.device(device))\n",
    "    test_emb = torch.tensor(test_emb, dtype = torch.float32, device=torch.device(device))\n",
    "    vals = []\n",
    "    inds = []\n",
    "    for test_batch in tqdm(test_emb.split(batchsize),disable=1-verbose):\n",
    "        sim_mat = cos_similarity_matrix(test_batch, tr_emb)\n",
    "        vals_batch, inds_batch = torch.topk(sim_mat, k=k, dim=1)\n",
    "        vals += [vals_batch.detach().cpu()]\n",
    "        inds += [inds_batch.detach().cpu()]\n",
    "    vals = torch.cat(vals)\n",
    "    inds = torch.cat(inds)\n",
    "    if save:\n",
    "        save_pickle(vals,'val_'+name)\n",
    "        save_pickle(inds,'ind_'+name)\n",
    "    gc.collect()\n",
    "    return vals, inds\n",
    "\n",
    "def get_topk_cossim_sub(test_emb, tr_emb, vals_x, batchsize = 64, k=10, device='cuda:0',verbose=True,save = False, name = 'simple'):\n",
    "    tr_emb = torch.tensor(tr_emb, dtype = torch.float32, device=torch.device(device))\n",
    "    test_emb = torch.tensor(test_emb, dtype = torch.float32, device=torch.device(device))\n",
    "    vals_x = torch.tensor(vals_x, dtype = torch.float32, device=torch.device(device))\n",
    "    vals = []\n",
    "    inds = []\n",
    "    for test_batch in tqdm(test_emb.split(batchsize),disable=1-verbose):\n",
    "        sim_mat = cos_similarity_matrix(test_batch, tr_emb)\n",
    "        sim_mat = torch.clamp(sim_mat,0,1) - vals_x.repeat(sim_mat.shape[0], 1)\n",
    "        \n",
    "        vals_batch, inds_batch = torch.topk(sim_mat, k=k, dim=1)\n",
    "        vals += [vals_batch.detach().cpu()]\n",
    "        inds += [inds_batch.detach().cpu()]\n",
    "    vals = torch.cat(vals)\n",
    "    inds = torch.cat(inds)\n",
    "    if save:\n",
    "        save_pickle(vals,'val_'+name)\n",
    "        save_pickle(inds,'ind_'+name)\n",
    "    gc.collect()\n",
    "    return vals, inds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f946fe0",
   "metadata": {
    "papermill": {
     "duration": 0.043983,
     "end_time": "2021-10-01T22:08:13.353499",
     "exception": false,
     "start_time": "2021-10-01T22:08:13.309516",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f86f218f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:08:13.453468Z",
     "iopub.status.busy": "2021-10-01T22:08:13.445263Z",
     "iopub.status.idle": "2021-10-01T22:08:13.456708Z",
     "shell.execute_reply": "2021-10-01T22:08:13.455980Z",
     "shell.execute_reply.started": "2021-10-01T21:30:29.384920Z"
    },
    "papermill": {
     "duration": 0.059183,
     "end_time": "2021-10-01T22:08:13.456842",
     "exception": false,
     "start_time": "2021-10-01T22:08:13.397659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_pickle(data, filename = 'tmp', save_joblib = False):\n",
    "    print('\\tSaving {} to {}'.format(filename.split('.')[0], filename))\n",
    "    try:\n",
    "        np.save(os.path.join(hyperparameters.CACHE_DIR,filename), data)\n",
    "    except:\n",
    "        np.save(os.path.join(hyperparameters.CACHE_DIR,filename), data.detach().cpu().numpy())\n",
    "    \n",
    "    del data\n",
    "    \n",
    "def load_pickle(filename):\n",
    "    print('\\tLoading {} from {}'.format(filename.split('.')[0], filename))\n",
    "    return np.load(os.path.join(hyperparameters.CACHE_DIR,filename))\n",
    "\n",
    "def index_df():\n",
    "    image_ids = []\n",
    "    for i in glob.glob('../input/landmark-retrieval-2021/index/*/*/*/*.jpg'):\n",
    "        image_ids.append(i.split('/')[-1].split('.')[0])\n",
    "    df = pd.DataFrame({'id':image_ids, 'landmark_id': [-2]*len(image_ids)})\n",
    "    del image_ids\n",
    "    return df\n",
    "\n",
    "def targets_prob():\n",
    "    p = np.load('../input/glr-files/classes.npy')\n",
    "    idx2landmark_id = {ix:x for ix, x in enumerate(p)}\n",
    "    landmark_id2idx = {x:ix for ix, x in enumerate(p)}\n",
    "    del p\n",
    "\n",
    "    pred_mask = pd.Series(df_train.landmark_id.unique()).map(landmark_id2idx).values\n",
    "    gc.collect()\n",
    "    return pred_mask, idx2landmark_id, landmark_id2idx "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f048ab03",
   "metadata": {
    "papermill": {
     "duration": 0.04323,
     "end_time": "2021-10-01T22:08:13.544143",
     "exception": false,
     "start_time": "2021-10-01T22:08:13.500913",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Compute inlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f08702d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:08:13.645479Z",
     "iopub.status.busy": "2021-10-01T22:08:13.644513Z",
     "iopub.status.idle": "2021-10-01T22:08:13.647476Z",
     "shell.execute_reply": "2021-10-01T22:08:13.648115Z",
     "shell.execute_reply.started": "2021-10-01T21:30:29.401288Z"
    },
    "papermill": {
     "duration": 0.05996,
     "end_time": "2021-10-01T22:08:13.648289",
     "exception": false,
     "start_time": "2021-10-01T22:08:13.588329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_INLIER_SCORE = 70\n",
    "MAX_REPROJECTION_ERROR = 4.0\n",
    "MAX_RANSAC_ITERATIONS = 1000\n",
    "HOMOGRAPHY_CONFIDENCE = 0.99\n",
    "\n",
    "def compute_putative_matching_keypoints(test_keypoints,\n",
    "                                        test_descriptors,\n",
    "                                        train_keypoints,\n",
    "                                        train_descriptors,\n",
    "                                        max_distance=0.9):\n",
    "    \"\"\"Finds matches from `test_descriptors` to KD-tree of `train_descriptors`.\"\"\"\n",
    "\n",
    "    train_descriptor_tree = spatial.cKDTree(train_descriptors)\n",
    "    _, matches = train_descriptor_tree.query(\n",
    "      test_descriptors, distance_upper_bound=max_distance)\n",
    "\n",
    "    test_kp_count = test_keypoints.shape[0]\n",
    "    train_kp_count = train_keypoints.shape[0]\n",
    "\n",
    "    test_matching_keypoints = np.array([\n",
    "      test_keypoints[i,]\n",
    "      for i in range(test_kp_count)\n",
    "      if matches[i] != train_kp_count\n",
    "    ])\n",
    "    train_matching_keypoints = np.array([\n",
    "      train_keypoints[matches[i],]\n",
    "      for i in range(test_kp_count)\n",
    "      if matches[i] != train_kp_count\n",
    "    ])\n",
    "    gc.collect()\n",
    "    return test_matching_keypoints, train_matching_keypoints\n",
    "\n",
    "def compute_num_inliers(test_keypoints, test_descriptors, train_keypoints,\n",
    "                        train_descriptors, do_kdtree=True):\n",
    "    \"\"\"Returns the number of RANSAC inliers.\"\"\"\n",
    "\n",
    "    if do_kdtree:\n",
    "        test_match_kp, train_match_kp = compute_putative_matching_keypoints(\n",
    "            test_keypoints, test_descriptors, train_keypoints, train_descriptors)\n",
    "    else:\n",
    "        test_match_kp, train_match_kp = test_keypoints, train_keypoints\n",
    "    if test_match_kp.shape[0] <= 4:  # Min keypoints supported by `pydegensac.findHomography()`\n",
    "        return 0\n",
    "\n",
    "    try:\n",
    "        _, mask = pydegensac.findHomography(test_match_kp, train_match_kp,\n",
    "                                        MAX_REPROJECTION_ERROR,\n",
    "                                        HOMOGRAPHY_CONFIDENCE,\n",
    "                                        MAX_RANSAC_ITERATIONS)\n",
    "    except np.linalg.LinAlgError:  # When det(H)=0, can't invert matrix.\n",
    "        return 0\n",
    "    gc.collect()\n",
    "    return int(copy.deepcopy(mask).astype(np.float32).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44aebb9",
   "metadata": {
    "papermill": {
     "duration": 0.044228,
     "end_time": "2021-10-01T22:08:13.736774",
     "exception": false,
     "start_time": "2021-10-01T22:08:13.692546",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc895236",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:08:13.852761Z",
     "iopub.status.busy": "2021-10-01T22:08:13.851999Z",
     "iopub.status.idle": "2021-10-01T22:08:41.118723Z",
     "shell.execute_reply": "2021-10-01T22:08:41.119220Z",
     "shell.execute_reply.started": "2021-10-01T21:30:29.416959Z"
    },
    "papermill": {
     "duration": 27.338424,
     "end_time": "2021-10-01T22:08:41.119407",
     "exception": false,
     "start_time": "2021-10-01T22:08:13.780983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded ../input/model/epoch_5_score_0.0.pth\n",
      "loaded ../input/model/epoch_3_score_0.0b3ns.pth\n"
     ]
    }
   ],
   "source": [
    "def load_model(model, model_file):\n",
    "    state_dict = torch.load(model_file)\n",
    "    if \"model_state_dict\" in state_dict.keys():\n",
    "        state_dict = state_dict[\"model_state_dict\"]\n",
    "    state_dict = {k[7:] if k.startswith('module.') else k: state_dict[k] for k in state_dict.keys()}\n",
    "#     del state_dict['final.weight']\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    print(f\"loaded {model_file}\")\n",
    "    del state_dict\n",
    "    model.eval()  \n",
    "    gc.collect()\n",
    "    return model\n",
    "\n",
    "# model1 = LandmarkNet(81313, backbone = 'tf_efficientnet_b3_ns', pool_type = 'gridgem', metric = 'curricular', pretrained = False)\n",
    "# model1 = load_model(model1,'../input/google-mark-no1/epoch_6_score_0.01662521950018494.pth').cuda()\n",
    "model1 = LandmarkNet(203094, backbone = 'tf_efficientnetv2_m_in21ft1k', pool_type = 'gridgem', metric = 'arcface', pretrained = False)\n",
    "model1 = load_model(model1,'../input/model/epoch_5_score_0.0.pth').cuda()\n",
    "\n",
    "# model2 = LandmarkNet(81313, backbone = 'tf_efficientnetv2_m_in21ft1k', pool_type = 'gridgem', metric = 'arcface', pretrained = False)\n",
    "# model2 = load_model(model2,'../input/model/epoch_9_score_0.1616932213254062 (1).pth').cuda()\n",
    "\n",
    "# model3 = LandmarkNet(81313, backbone = 'tf_efficientnet_b3_ns', pool_type = 'gridgem', metric = 'curricular', pretrained = False)\n",
    "# model3 = load_model(model3,'../input/model/epoch_10_score_0.16941185050639018.pth').cuda()\n",
    "\n",
    "model4 = LandmarkNet(203094, backbone = 'tf_efficientnet_b3_ns', pool_type = 'gridgem', metric = 'curricular', pretrained = False)\n",
    "model4 = load_model(model4,'../input/model/epoch_3_score_0.0b3ns.pth').cuda()\n",
    "\n",
    "# model1 = LandmarkNet(81313, backbone = 'tf_efficientnet_b3', pool_type = 'gem',sub_center = True, pretrained = False)\n",
    "# model1 = load_model(model1,'../input/morizin-exp1/epoch_10_score_0.001845540972934233.pth').cuda()\n",
    "# model2 = LandmarkNet(81313, backbone = 'tf_efficientnet_b3', pool_type = 'gem',sub_center = True, pretrained = False)\n",
    "# model2 = load_model(model2,'../input/morizin-exp1/epoch_9_score_0.0015537621993018456.pth').cuda()\n",
    "# models = [model1, model2, model3, model4]\n",
    "models = [model1, model4,]\n",
    "del model1, model4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfe1efe",
   "metadata": {
    "papermill": {
     "duration": 0.045402,
     "end_time": "2021-10-01T22:08:41.209156",
     "exception": false,
     "start_time": "2021-10-01T22:08:41.163754",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50d4efed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:08:41.317241Z",
     "iopub.status.busy": "2021-10-01T22:08:41.316123Z",
     "iopub.status.idle": "2021-10-01T22:08:41.319447Z",
     "shell.execute_reply": "2021-10-01T22:08:41.318942Z",
     "shell.execute_reply.started": "2021-10-01T21:31:07.957890Z"
    },
    "papermill": {
     "duration": 0.065134,
     "end_time": "2021-10-01T22:08:41.319580",
     "exception": false,
     "start_time": "2021-10-01T22:08:41.254446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_prediction(mode = 'retrieval'):\n",
    "    if mode == 'retrieval':\n",
    "        train_image_dir = f'../input/landmark-retrieval-2021/index'\n",
    "        test_image_dir = f'../input/landmark-retrieval-2021/test'\n",
    "    elif mode == 'recognition':\n",
    "        train_image_dir = f'../input/landmark-recognition-2021/train'\n",
    "        test_image_dir = f'../input/landmark-recognition-2021/test'\n",
    "    \n",
    "    df_train = pd.read_csv(f'../input/landmark-{mode}-2021/train.csv')\n",
    "    df_test  = pd.read_csv(f'../input/landmark-{mode}-2021/sample_submission.csv')\n",
    "    df_nl    = pd.read_csv('../input/google-landmark-2021-validation/valid.csv').sample(50000).reset_index(drop = True)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    if mode == 'retrieval':\n",
    "        if len(df_test) == 1129:\n",
    "            shutil.copy('../input/landmark-retrieval-2021/sample_submission.csv','./submission.csv')\n",
    "            return './submission.csv'\n",
    "        \n",
    "        df_index = index_df()\n",
    "        train_ids, train_embeddings = extract_global_features(df_index, mode = 'index', prob_type = mode)\n",
    "#         _, train_embeddings2 = extract_global_features_tf(df_index, train_image_dir)\n",
    "        del df_index\n",
    "    else:\n",
    "        if len(df_test) == 10345:\n",
    "            shutil.copy('../input/landmark-recognition-2021/sample_submission.csv','./submission.csv')\n",
    "            return './submission.csv'\n",
    "        train_ids, train_embeddings = extract_global_features(df_train, mode = 'train', prob_type = mode)\n",
    "#         _, train_embeddings2 = extract_global_features_tf(df_train, train_image_dir)\n",
    "    \n",
    "    nolandmark_ids, nolandmark_embeddings = extract_global_features(df_nl, mode = 'nolandmark', prob_type = mode)\n",
    "#     _, nolandmark_embeddings2 = extract_global_features_tf(df_nl, '../input/google-landmark-2021-validation/valid')\n",
    "    \n",
    "    test_ids, test_embeddings = extract_global_features(df_test, mode = 'test', prob_type = mode)\n",
    "#     _, test_embeddings2 = extract_global_features_tf(df_test,test_image_dir)\n",
    "    \n",
    "#     train_embeddings = np.concatenate([train_embeddings, train_embeddings2], axis = 1)\n",
    "#     nolandmark_embeddings = np.concatenate([nolandmark_embeddings, nolandmark_embeddings2], axis = 1)\n",
    "#     test_embeddings = np.concatenate([test_embeddings, test_embeddings2], axis = 1)\n",
    "    \n",
    "    del  df_nl\n",
    "#     , train_embeddings2, test_embeddings2, nolandmark_embeddings2\n",
    "    memory_cleanup()\n",
    "    del globals()['models']\n",
    "    \n",
    "    if mode == 'retrieval':\n",
    "        labelmap = dict([(i, -2) for i in train_ids])\n",
    "    else:\n",
    "        labelmap = load_labelmap('../input/landmark-recognition-2021/train.csv')\n",
    "    nolandmark_labelmap = dict([(i, -1) for i in nolandmark_ids])\n",
    "    \n",
    "    memory_cleanup()\n",
    "    \n",
    "    _, val_x = do_retrieval(nolandmark_labelmap, nolandmark_ids,\n",
    "                                                    nolandmark_embeddings, train_embeddings,\n",
    "                                                    5, gallery_set='nolandmark',return_vals = True, name = 'train_nl_sim')\n",
    "    \n",
    "    _, val_y = do_retrieval(nolandmark_labelmap, nolandmark_ids,\n",
    "                                                    nolandmark_embeddings, test_embeddings,\n",
    "                                                    11, gallery_set='nolandmark',return_vals = True, name = 'test_nl_sim')\n",
    "    del nolandmark_embeddings, nolandmark_ids, nolandmark_labelmap\n",
    "    \n",
    "    train_ids_labels_and_scores = do_retrieval(labelmap, train_ids,\n",
    "                                               train_embeddings, test_embeddings,\n",
    "                                               25, gallery_set='train', save = True, name= 'train_test_sim')\n",
    "    \n",
    "#     df_pair = do_gcn(df_train,df_test, 25, 25)\n",
    "    del df_train,df_test\n",
    "    \n",
    "    train_ids_labels_and_scores = do_retrieval(labelmap, train_ids,\n",
    "                                               train_embeddings, test_embeddings,\n",
    "                                               3,val_x = val_x[:,:].mean(axis=1).detach().cpu().numpy(),\n",
    "                                               val_y = None, gallery_set='train', name= 'train_test_sim')\n",
    "    \n",
    "    \n",
    "    \n",
    "    memory_cleanup()\n",
    "    del train_embeddings, test_embeddings, val_y, val_x, labelmap\n",
    "    memory_cleanup()\n",
    "    submit_fname = save_submission_csv(f'/kaggle/input/landmark-{mode}-2021', test_ids = test_ids, predictions=train_ids_labels_and_scores, mode = mode,)\n",
    "#     save_pickle(train_ids_labels_and_scores, 'predicition_v1.pkl')\n",
    "    del train_ids_labels_and_scores\n",
    "    memory_cleanup()\n",
    "    return submit_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f109a2bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:08:41.413196Z",
     "iopub.status.busy": "2021-10-01T22:08:41.412244Z",
     "iopub.status.idle": "2021-10-01T22:08:43.096182Z",
     "shell.execute_reply": "2021-10-01T22:08:43.095605Z",
     "shell.execute_reply.started": "2021-10-01T21:31:07.976259Z"
    },
    "papermill": {
     "duration": 1.73366,
     "end_time": "2021-10-01T22:08:43.096333",
     "exception": false,
     "start_time": "2021-10-01T22:08:41.362673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub_v1    = pd.read_csv(get_prediction(mode = 'recognition'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c740c5ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-01T22:08:43.195383Z",
     "iopub.status.busy": "2021-10-01T22:08:43.194074Z",
     "iopub.status.idle": "2021-10-01T22:08:43.212495Z",
     "shell.execute_reply": "2021-10-01T22:08:43.211662Z",
     "shell.execute_reply.started": "2021-10-01T21:31:14.153409Z"
    },
    "papermill": {
     "duration": 0.073481,
     "end_time": "2021-10-01T22:08:43.212631",
     "exception": false,
     "start_time": "2021-10-01T22:08:43.139150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>landmarks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00084cdf8f600d00</td>\n",
       "      <td>137790 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000b15b043eb8cf0</td>\n",
       "      <td>137790 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0011a52f9b948fd2</td>\n",
       "      <td>137790 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00141b8a5a729084</td>\n",
       "      <td>137790 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0018aa4b92532b77</td>\n",
       "      <td>137790 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10340</th>\n",
       "      <td>ffc41ddbcf63289b</td>\n",
       "      <td>137790 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10341</th>\n",
       "      <td>ffd4c19b7732cbe9</td>\n",
       "      <td>137790 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10342</th>\n",
       "      <td>ffee2cac79a173d6</td>\n",
       "      <td>137790 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10343</th>\n",
       "      <td>ffef459d5dc6b981</td>\n",
       "      <td>137790 0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10344</th>\n",
       "      <td>fff30527c712f754</td>\n",
       "      <td>137790 0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10345 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id   landmarks\n",
       "0      00084cdf8f600d00  137790 0.1\n",
       "1      000b15b043eb8cf0  137790 0.1\n",
       "2      0011a52f9b948fd2  137790 0.1\n",
       "3      00141b8a5a729084  137790 0.1\n",
       "4      0018aa4b92532b77  137790 0.1\n",
       "...                 ...         ...\n",
       "10340  ffc41ddbcf63289b  137790 0.1\n",
       "10341  ffd4c19b7732cbe9  137790 0.1\n",
       "10342  ffee2cac79a173d6  137790 0.1\n",
       "10343  ffef459d5dc6b981  137790 0.1\n",
       "10344  fff30527c712f754  137790 0.1\n",
       "\n",
       "[10345 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506550fb",
   "metadata": {
    "papermill": {
     "duration": 0.045826,
     "end_time": "2021-10-01T22:08:43.304132",
     "exception": false,
     "start_time": "2021-10-01T22:08:43.258306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c5c62",
   "metadata": {
    "papermill": {
     "duration": 0.043702,
     "end_time": "2021-10-01T22:08:43.393117",
     "exception": false,
     "start_time": "2021-10-01T22:08:43.349415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0f513d",
   "metadata": {
    "papermill": {
     "duration": 0.044567,
     "end_time": "2021-10-01T22:08:43.481531",
     "exception": false,
     "start_time": "2021-10-01T22:08:43.436964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1ce485",
   "metadata": {
    "papermill": {
     "duration": 0.043829,
     "end_time": "2021-10-01T22:08:43.567732",
     "exception": false,
     "start_time": "2021-10-01T22:08:43.523903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df731c75",
   "metadata": {
    "papermill": {
     "duration": 0.042819,
     "end_time": "2021-10-01T22:08:43.654180",
     "exception": false,
     "start_time": "2021-10-01T22:08:43.611361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 117.899673,
   "end_time": "2021-10-01T22:08:45.589735",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-10-01T22:06:47.690062",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
